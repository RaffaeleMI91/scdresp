{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8bf9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.colors as clrs\n",
    "from IPython.core.display import display, HTML\n",
    "import anndata\n",
    "from sklearn.metrics import silhouette_score\n",
    "from joblib import parallel_backend\n",
    "import math\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b826bfca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df7254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fbff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.settings.verbosity=0\n",
    "sc.settings.set_figure_params(dpi=100)\n",
    "sc.logging.print_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9617ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(0, (\"#E0E0E0\")), (0.25, (\"#F1F1F1\")), (1,(\"#FD0D0D\"))]\n",
    "n_bins = 250 #Â discretizes the interpolation into bins\n",
    "cmap_name = \"new_list\"\n",
    "cm_2 = clrs.LinearSegmentedColormap.from_list(cmap_name, colors , N = n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b86cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\"<style>.container { width: 90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62496b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdscat2=pd.read_csv(\"/group/iorio/Raffaele/SCDRESP_data/data/gdsc/GDSCatSquare-009_matrix_results.csv\")\n",
    "gdsc=pd.read_excel(\"/group/iorio/Raffaele/SCDRESP_data/data/gdsc/GDSC2_fitted_dose_response.xlsx\")\n",
    "gdsc.rename(columns={\"SANGER_MODEL_ID\":\"SangerModelID\"},inplace=True)\n",
    "gex=pd.read_csv(\"/group/iorio/Raffaele/SCDRESP_data/data/ccle/OmicsExpressionProteinCodingGenesTPMLogp1BatchCorrected.csv\",index_col=0)\n",
    "genes=[re.sub(r\"\\s*\\(.*?\\)\", \"\", gene) for gene in gex.columns]\n",
    "gex.columns=genes\n",
    "gex=gex.reset_index().rename(columns={\"index\":\"ModelID\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6750d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_broad=pd.read_csv(\"/group/iorio/Raffaele/SCDRESP_data/data/Model.csv\")\n",
    "model_broad=model_broad[[\"ModelID\",\"SangerModelID\",\"OncotreeSubtype\"]]\n",
    "model_broad=model_broad[model_broad[\"SangerModelID\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34af8258",
   "metadata": {},
   "outputs": [],
   "source": [
    "gex=gex[gex[\"ModelID\"].isin(model_broad[\"ModelID\"].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76e7805",
   "metadata": {},
   "outputs": [],
   "source": [
    "gex_extended = gex.merge(model_broad, on=\"ModelID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c54a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/group/iorio/Raffaele/SCDRESP_data/data/adata_dict.pkl\", \"rb\") as f:\n",
    "    adata_paths=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01603906",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_adata_cl_dict={key: anndata.read_h5ad(path) for key, path in adata_paths.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7397357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_lengths=pd.read_csv(\"/group/iorio/Raffaele/SCDRESP_data/data/gene_lengths.csv\", index_col=0)\n",
    "gene_lengths.set_index(\"Gene_Name\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c006884",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/group/iorio/Raffaele/SCDRESP_data/data/filtered_clustering_res.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f767ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_adata_aggrGEPs_cl_dict = {}\n",
    "\n",
    "for idx, CL in enumerate(sub_adata_cl_dict.keys()):\n",
    "    print(f'{idx} : {CL}')\n",
    "    adata = sub_adata_cl_dict[CL].copy()  \n",
    "    cell_data = df.loc[df[\"CellLine\"] == CL].iloc[0]\n",
    "    nHVG = cell_data[\"nHVG\"]\n",
    "    method = cell_data[\"cl_method\"]\n",
    "    var = cell_data[\"res\"] if method == \"leiden\" else cell_data[\"cl\"]\n",
    "    cl_key = f\"{method}_{nHVG}_{var}_clusters\"\n",
    "\n",
    "    counts_df = pd.DataFrame(adata.layers[\"counts\"].toarray() if hasattr(adata.layers[\"counts\"], \"toarray\") else adata.layers[\"counts\"],\n",
    "                             index=adata.obs_names, \n",
    "                             columns=adata.var_names)\n",
    "    \n",
    "    aggr_counts_df = counts_df.groupby(adata.obs[cl_key]).sum()\n",
    "    GENES = aggr_counts_df.columns.intersection(gene_lengths.index)\n",
    "    \n",
    "    LEN_GENES = gene_lengths[gene_lengths.index.isin(GENES)][\"Length\"]\n",
    "    LEN_GENES = LEN_GENES[~LEN_GENES.index.duplicated(keep=\"first\")]\n",
    "    \n",
    "    aggr_counts_df = aggr_counts_df.loc[:,GENES]\n",
    "    rpk = aggr_counts_df.div(LEN_GENES, axis=1)  # Reads Per Kilobase\n",
    "    total_rpk = rpk.sum(axis=1)  # Sum of all RPK values per sample\n",
    "    tpm = rpk.div(total_rpk, axis=0) * 1e6  # Scale to TPM\n",
    "    adata_aggr = sc.AnnData(tpm)\n",
    "    sub_adata_aggrGEPs_cl_dict[CL] = adata_aggr\n",
    "\n",
    "adata_combined = sc.concat(list(sub_adata_aggrGEPs_cl_dict.values()), join=\"inner\", label=\"ID\", keys=list(sub_adata_aggrGEPs_cl_dict.keys()))\n",
    "sc_TPM = pd.DataFrame(adata_combined.X, index=adata_combined.obs_names, columns=adata_combined.var_names)\n",
    "sc_TPM[\"CellLine\"] = adata_combined.obs[\"ID\"].values\n",
    "sc_TPM[\"Clone_ID\"] = sc_TPM[\"CellLine\"].astype(str) + \"_clone_\" + sc_TPM.index.astype(str)\n",
    "sc_TPM.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe52cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_A=sc_TPM.iloc[:,:-2]\n",
    "dataset_B=gex_extended.iloc[:,1:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5f1c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_in_common = list(set(dataset_A.columns) & set(dataset_B.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f365f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_A=dataset_A.loc[:,genes_in_common]\n",
    "dataset_A=np.log1p(dataset_A)+1\n",
    "dataset_B=dataset_B.loc[:,genes_in_common]\n",
    "dataset_A[\"type\"] = \"sc pseudobulk\"\n",
    "dataset_A[\"SangerModelID\"] = sc_TPM[\"CellLine\"]\n",
    "dataset_B[\"type\"] = \"bulk\"\n",
    "dataset_B[\"SangerModelID\"] = gex_extended[\"SangerModelID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcd84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_C = pd.concat([dataset_A, dataset_B])\n",
    "dataset_C = dataset_C.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce2b536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Standardize the dataset\n",
    "scaler = StandardScaler()\n",
    "scaled_C_df = scaler.fit_transform(dataset_C.iloc[:, :-2])\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "pc_transformed = pca.fit_transform(scaled_C_df)\n",
    "\n",
    "# Compute variance explained in percentage\n",
    "explained_variance = pca.explained_variance_ratio_ * 100\n",
    "\n",
    "# Create DataFrame with labeled PCs including variance explained\n",
    "df_pca = pd.DataFrame(pc_transformed, columns=[f\"PC{i+1} ({var:.2f}%)\" for i, var in enumerate(explained_variance)])\n",
    "\n",
    "# Reset index for merging with dataset_C\n",
    "df_pca = df_pca.reset_index(drop=True)\n",
    "dataset_C = dataset_C.reset_index(drop=True)\n",
    "\n",
    "# Add back the \"type\" column\n",
    "df_pca[\"type\"] = dataset_C[\"type\"]\n",
    "\n",
    "# Plot PCA with variance explained in axis labels\n",
    "fig, ax = plt.subplots(figsize=(8,6))  \n",
    "sns.scatterplot(data=df_pca, x=df_pca.columns[0], y=df_pca.columns[1], hue=\"type\", alpha=0.8, ax=ax)\n",
    "plt.title(\"PCA of Gene Expression Data (pre correction)\")\n",
    "plt.legend(title=\"Tissue Type\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "pdf_path = \"/group/iorio/Raffaele/SCDRESP_data/data/pca_precorrection.pdf\"\n",
    "\n",
    "fig.savefig(pdf_path, format='pdf', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5607ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from combat.pycombat import pycombat\n",
    "corrected = pycombat(dataset_C.iloc[:,:-2].T, dataset_C[\"type\"])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_corrected_df = scaler.fit_transform(corrected.T)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pc_transformed = pca.fit_transform(scaled_corrected_df.T)\n",
    "\n",
    "# Compute variance explained in percentage\n",
    "explained_variance = pca.explained_variance_ratio_ * 100\n",
    "\n",
    "# Create DataFrame with labeled PCs including variance explained\n",
    "df_pca = pd.DataFrame(pc_transformed, columns=[f\"PC{i+1} ({var:.2f}%)\" for i, var in enumerate(explained_variance)])\n",
    "\n",
    "# Reset index for merging with dataset_C\n",
    "df_pca = df_pca.reset_index(drop=True)\n",
    "dataset_C = dataset_C.reset_index(drop=True)\n",
    "\n",
    "df_pca[\"type\"] = dataset_C[\"type\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.scatterplot(data=df_pca, x=df_pca.columns[0], y=df_pca.columns[1], hue=\"type\", alpha=0.8, ax=ax)\n",
    "plt.title(\"PCA of Gene Expression Data (post correction)\")\n",
    "plt.legend(title=\"Tissue Type\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "pdf_path = \"/group/iorio/Raffaele/SCDRESP_data/data/pca_postcorrection.pdf\"\n",
    "fig.savefig(pdf_path, format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f8832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected = corrected.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fad0061",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected[\"type\"] = dataset_C[\"type\"]\n",
    "corrected[\"SangerModelID\"] = dataset_C[\"SangerModelID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff3b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "gex_corrected = corrected[corrected[\"type\"] == \"bulk\"]\n",
    "gex_corrected = gex_corrected.drop(columns=\"type\").reset_index(drop=True)\n",
    "gex_corrected[\"OncotreeSubtype\"] = gex_extended[\"OncotreeSubtype\"]\n",
    "gex_corrected = gex_corrected.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc0487",
   "metadata": {},
   "outputs": [],
   "source": [
    "gex_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df75d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs = {'AZD5153',\n",
    " 'AZD5991',\n",
    " 'AZD6738',\n",
    " 'Crizotinib',\n",
    " 'Dasatinib',\n",
    " 'Gefitinib',\n",
    " 'Lapatinib',\n",
    " 'MK-2206',\n",
    " 'Palbociclib',\n",
    " 'Savolitinib',\n",
    " 'Selumetinib',\n",
    " 'Trametinib',\n",
    " 'Venetoclax'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194818c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLs_from_gex = set(gex_corrected[\"SangerModelID\"].unique()) \n",
    "GENES = gex_corrected.drop(columns=\"SangerModelID\").columns\n",
    "\n",
    "training_sets_dict = {}\n",
    "\n",
    "for dname in drugs:\n",
    "    dresp_sub_df = gdsc[gdsc[\"DRUG_NAME\"] == dname]\n",
    "    CLs_from_dresp = set(dresp_sub_df[\"SangerModelID\"].unique())\n",
    "    CLs_to_keep = CLs_from_dresp & CLs_from_gex \n",
    "    dresp_filtered = dresp_sub_df[dresp_sub_df[\"SangerModelID\"].isin(CLs_to_keep)]\n",
    "    gex_filtered = gex_corrected[gex_corrected[\"SangerModelID\"].isin(CLs_to_keep)]\n",
    "    df = gex_filtered.merge(dresp_filtered, on=\"SangerModelID\", how=\"left\")\n",
    "    ctype_counts = df[\"OncotreeSubtype\"].value_counts()\n",
    "    ctype_counts = ctype_counts[ctype_counts > 10].index # select ctype with at least 10 cell lines each\n",
    "    df = df[df[\"OncotreeSubtype\"].isin(ctype_counts)]\n",
    "    SAMPLES = df[\"SangerModelID\"].reset_index(drop=True)\n",
    "    GEX=df.loc[:,df.columns.isin(GENES)].reset_index(drop=True)\n",
    "    RESP=df[\"LN_IC50\"].reset_index(drop=True)\n",
    "    META=df[[\"OncotreeSubtype\",\"CELL_LINE_NAME\",\"PATHWAY_NAME\",\"COSMIC_ID\"]].reset_index(drop=True)\n",
    "    training_sets_dict[dname] = {\"OBS\":SAMPLES, \"X\":GEX, \"Y\":RESP, \"Metadata\":META}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075150c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ncols = 3  \n",
    "nrows = math.ceil(len(training_sets_dict) / ncols)  \n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(20, 25))\n",
    "axes = axes.flatten()\n",
    "for idx, (drug, data) in enumerate(training_sets_dict.items()):\n",
    "    df = data[\"Metadata\"][\"OncotreeSubtype\"]\n",
    "    sns.barplot(x=df.value_counts().index, y=df.value_counts().values, ax=axes[idx])\n",
    "    axes[idx].set_xticklabels(axes[idx].get_xticklabels(), rotation=45, ha=\"right\", fontsize=12)\n",
    "    axes[idx].tick_params(axis=\"y\", labelsize=12)\n",
    "    axes[idx].set_title(f\"{drug}; nr of cell lines: {len(data['OBS'])}\", fontsize=14)\n",
    "\n",
    "for idx in range(len(drugs), len(axes)):\n",
    "    axes[idx].set_visible(False)  \n",
    "\n",
    "plt.subplots_adjust(hspace=0.8, wspace=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cb0c0a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "perc = 90\n",
    "ncols = 3  \n",
    "nrows = math.ceil(len(training_sets_dict) / ncols)  \n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(20, 25))\n",
    "axes = axes.flatten()\n",
    "for idx, (drug, data) in enumerate(training_sets.items()):\n",
    "    df = data[\"Metadata\"][\"OncotreeSubtype\"]\n",
    "    threshold = np.percentile(df.value_counts(), perc)\n",
    "    counts_df = df.value_counts().reset_index()\n",
    "    counts_df.columns = [\"Subtype\", \"Count\"]\n",
    "    counts_df[\"Class\"] = np.nan\n",
    "    counts_df.loc[counts_df[\"Count\"] >= threshold, \"Class\"] = \"over\"\n",
    "    counts_df.loc[counts_df[\"Count\"] < threshold, \"Class\"] = \"down\"\n",
    "    sns.barplot(x=counts_df[\"Subtype\"], y=counts_df[\"Count\"], hue=counts_df[\"Class\"], ax=axes[idx])\n",
    "    axes[idx].set_xticklabels(axes[idx].get_xticklabels(), rotation=45, ha=\"right\", fontsize=12)\n",
    "    axes[idx].tick_params(axis=\"y\", labelsize=12)\n",
    "    axes[idx].set_title(f\"{drug}; nr of cell lines: {len(data['OBS'])}\", fontsize=14)\n",
    "for idx in range(len(drug), len(axes)):\n",
    "    axes[idx].set_visible(False)  \n",
    "\n",
    "plt.subplots_adjust(hspace=0.8, wspace=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5590ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for idx, (drug, data) in enumerate(training_sets.items()):\n",
    "#    df = data[\"Metadata\"][\"OncotreeSubtype\"]\n",
    "#    threshold = np.percentile(df.value_counts(), perc)\n",
    "#    counts_df = df.value_counts().reset_index()\n",
    "#    counts_df.columns = [\"Subtype\", \"Count\"]\n",
    "#    counts_df[\"Class\"] = np.where(counts_df[\"Count\"] >= threshold, \"over\", \"down\")\n",
    "#    training_sets[drug][\"Metadata\"] = training_sets[drug][\"Metadata\"].copy()  \n",
    "#    training_sets[drug][\"Metadata\"][\"Class\"] = df.map(counts_df.set_index(\"Subtype\")[\"Class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab603f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CategorizeOTS(data_dict, perc=90, plot_figure=True):\n",
    "    \n",
    "    import numpy as np\n",
    "    import math\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    ncols = 3\n",
    "    nrows = math.ceil(len(data_dict) / ncols)  \n",
    "    \n",
    "    if plot_figure:\n",
    "        fig, axes = plt.subplots(nrows, ncols, figsize=(20, 25))\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for idx, (drug, data) in enumerate(data_dict.items()):\n",
    "        df = data[\"Metadata\"][\"OncotreeSubtype\"]\n",
    "        threshold = np.percentile(df.value_counts(), perc)\n",
    "        counts_df = df.value_counts().reset_index()\n",
    "        counts_df.columns = [\"Subtype\", \"Count\"]\n",
    "        \n",
    "        counts_df[\"Class\"] = np.where(counts_df[\"Count\"] >= threshold, \"over\", \"down\")\n",
    "\n",
    "        data_dict[drug][\"Metadata\"] = data[\"Metadata\"].copy()  \n",
    "        data_dict[drug][\"Metadata\"][\"Class\"] = df.map(counts_df.set_index(\"Subtype\")[\"Class\"])\n",
    "\n",
    "        if plot_figure:\n",
    "            sns.barplot(x=counts_df[\"Subtype\"], y=counts_df[\"Count\"], hue=counts_df[\"Class\"], ax=axes[idx])\n",
    "            axes[idx].set_xticklabels(axes[idx].get_xticklabels(), rotation=45, ha=\"right\", fontsize=12)\n",
    "            axes[idx].tick_params(axis=\"y\", labelsize=12)\n",
    "            axes[idx].set_title(f\"{drug}; nr of cell lines: {len(data['OBS'])}\", fontsize=14)\n",
    "\n",
    "    if plot_figure:\n",
    "        for idx in range(len(data_dict), len(axes)):\n",
    "            axes[idx].set_visible(False)\n",
    "\n",
    "        plt.subplots_adjust(hspace=0.8, wspace=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f34928",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CategorizeOTS(training_sets_dict,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdfb075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DownsampleOTS(data_dict):\n",
    "    \n",
    "    import numpy as np\n",
    "    import warnings\n",
    "    from sklearn.utils import resample\n",
    "    \n",
    "    \"\"\"\n",
    "    Downsamples the 'over' class in the 'OncotreeSubtype' column to balance the dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    group_idx_downsampled = {}\n",
    "\n",
    "    if not any(\"Class\" in data[\"Metadata\"].columns for data in data_dict.values()):\n",
    "        warnings.warn(\"Column 'Class' is missing. Running CategorizeOTS() to categorize data.\", UserWarning)\n",
    "        CategorizeOTS(data_dict, plot_figure=False)\n",
    "\n",
    "    for drug, data in data_dict.items():\n",
    "        \n",
    "        group_idx_downsampled[drug] = {}\n",
    "        \n",
    "        mdata = data[\"Metadata\"]\n",
    "\n",
    "        if \"Class\" not in mdata.columns:\n",
    "            raise ValueError(f\"Column 'Class' is still missing in metadata for {drug} after CategorizeOTS().\")\n",
    "\n",
    "        maj_class = mdata[mdata[\"Class\"] == \"over\"].index\n",
    "        min_class = mdata[mdata[\"Class\"] == \"down\"].index\n",
    "\n",
    "        # Extract corresponding metadata\n",
    "        min_data = mdata.loc[min_class]\n",
    "        maj_data = mdata.loc[maj_class]\n",
    "\n",
    "        group_idx = maj_data.groupby(\"OncotreeSubtype\").groups\n",
    "        \n",
    "        N = np.ceil(min_data[\"OncotreeSubtype\"].value_counts().mean()).astype(int)\n",
    "        \n",
    "        # Downsample each group ensuring no errors when N > available samples\n",
    "        group_idx_downsampled[drug] = {\n",
    "            gn: resample(group, replace=False, n_samples=min(N.astype(int), len(group)), random_state=42)\n",
    "            for gn, group in group_idx.items()\n",
    "        }\n",
    "    \n",
    "    idx_dict = {k: [idx for i_v in v.values() for idx in i_v] for k, v in group_idx_downsampled.items()}\n",
    "    return(idx_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f14c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DownsampleOTS(data_dict):\n",
    "    \n",
    "    import numpy as np\n",
    "    import warnings\n",
    "    from sklearn.utils import resample\n",
    "    \n",
    "    \"\"\"\n",
    "    Downsamples the 'over' class in the 'OncotreeSubtype' column to balance the dataset.\n",
    "    The number of retained samples in 'over' tissues is set to the mean count of 'down' tissues.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dict (dict): Dictionary containing datasets with Metadata.\n",
    "\n",
    "    Returns:\n",
    "    - Updated data_dict with downsampled `Balanced_Metadata`, `Balanced_X`, `Balanced_Y`\n",
    "    - Dictionary of downsampled indexes for tracking.\n",
    "    \"\"\"\n",
    "\n",
    "    group_idx_downsampled = {}\n",
    "\n",
    "    if not any(\"Class\" in data[\"Metadata\"].columns for data in data_dict.values()):\n",
    "        warnings.warn(\"Column 'Class' is missing. Running CategorizeOTS() to categorize data.\", UserWarning)\n",
    "        CategorizeOTS(data_dict, plot_figure=False)  # Ensure tissues are classified before downsampling\n",
    "\n",
    "    for drug, data in data_dict.items():\n",
    "        \n",
    "        group_idx_downsampled[drug] = {}\n",
    "        \n",
    "        mdata = data[\"Metadata\"]\n",
    "\n",
    "        if \"Class\" not in mdata.columns:\n",
    "            raise ValueError(f\"Column 'Class' is still missing in metadata for {drug} after CategorizeOTS().\")\n",
    "\n",
    "        maj_class = mdata[mdata[\"Class\"] == \"over\"].index\n",
    "        min_class = mdata[mdata[\"Class\"] == \"down\"].index\n",
    "\n",
    "        min_data = mdata.loc[min_class]\n",
    "        maj_data = mdata.loc[maj_class]\n",
    "\n",
    "        # Compute the mean sample count for downrepresented tissues\n",
    "        N = int(min_data[\"OncotreeSubtype\"].value_counts().mean())\n",
    "\n",
    "        # Group overrepresented tissues by subtype\n",
    "        group_idx = maj_data.groupby(\"OncotreeSubtype\").groups\n",
    "\n",
    "        downsampled_idx = list(min_class) \n",
    "\n",
    "        for tissue, indices in group_idx.items():\n",
    "            if len(indices) > N:\n",
    "                sampled_idx = resample(indices, n_samples=N, random_state=42, replace=False)\n",
    "            else:\n",
    "                sampled_idx = indices\n",
    "            \n",
    "            downsampled_idx.extend(sampled_idx)\n",
    "\n",
    "        # Store the downsampled indexes for this drug\n",
    "        group_idx_downsampled[drug] = downsampled_idx\n",
    "\n",
    "        # Apply downsampling to the dataset\n",
    "        data_dict[drug][\"Balanced_Metadata\"] = mdata.loc[downsampled_idx].copy()\n",
    "        data_dict[drug][\"Balanced_X\"] = data[\"X\"].loc[downsampled_idx].copy()\n",
    "        data_dict[drug][\"Balanced_Y\"] = data[\"Y\"].loc[downsampled_idx].copy()\n",
    "        data_dict[drug][\"Balanced_OBS\"] = data['OBS'].loc[downsampled_idx].copy()\n",
    "\n",
    "    return data_dict, group_idx_downsampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d8e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def BalanceOTS(data_dict, method=\"downsample\", apply_smote=False, n_components=10):\n",
    "    \"\"\"\n",
    "    Balances tissue representation in training datasets.\n",
    "\n",
    "    Options:\n",
    "    - Downsampling of overrepresented tissues to the mean of downrepresented tissues.\n",
    "    - Stratified downsampling, preserving gene expression variance.\n",
    "    - Optional SMOTE to oversample underrepresented tissues.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dict (dict): Dictionary containing datasets with Metadata.\n",
    "    - method (str): \"downsample\" (default) or \"stratified\" for stratified variance-based downsampling.\n",
    "    - apply_smote (bool): If True, applies SMOTE to underrepresented tissues.\n",
    "    - n_components (int): Number of PCA components to use for variance-based stratified downsampling.\n",
    "\n",
    "    Returns:\n",
    "    - Updated `data_dict` with:\n",
    "        - Balanced_Metadata (metadata of downsampled set)\n",
    "        - Balanced_X (gene expression data of balanced set)\n",
    "        - Balanced_Y (drug response data of balanced set)\n",
    "        - Balanced_OBS (indexes of selected cell lines)\n",
    "    - Dictionary of selected indexes for each drug.\n",
    "    \"\"\"\n",
    "\n",
    "    group_idx_balanced = {}\n",
    "\n",
    "    # Check if classification exists\n",
    "    if not any(\"Class\" in data[\"Metadata\"].columns for data in data_dict.values()):\n",
    "        warnings.warn(\"Column 'Class' is missing. Running CategorizeOTS() to categorize data.\", UserWarning)\n",
    "        CategorizeOTS(data_dict, plot_figure=False)  # Ensure tissues are classified before balancing\n",
    "\n",
    "    for drug, data in data_dict.items():\n",
    "        group_idx_balanced[drug] = {}\n",
    "        mdata = data[\"Metadata\"]\n",
    "\n",
    "        # Ensure classification column exists\n",
    "        if \"Class\" not in mdata.columns:\n",
    "            raise ValueError(f\"Column 'Class' is missing in metadata for {drug} after CategorizeOTS().\")\n",
    "\n",
    "        # Identify overrepresented and downrepresented tissue indices\n",
    "        maj_class = mdata[mdata[\"Class\"] == \"over\"].index\n",
    "        min_class = mdata[mdata[\"Class\"] == \"down\"].index\n",
    "\n",
    "        # Extract metadata\n",
    "        min_data = mdata.loc[min_class]\n",
    "        maj_data = mdata.loc[maj_class]\n",
    "\n",
    "        # Compute `N`: Average number of samples in downrepresented tissues\n",
    "        N = int(min_data[\"OncotreeSubtype\"].value_counts().mean())\n",
    "\n",
    "        # Group overrepresented tissues by subtype\n",
    "        group_idx = maj_data.groupby(\"OncotreeSubtype\").groups\n",
    "\n",
    "        balanced_idx = list(min_class)  # Keep all downrepresented samples\n",
    "\n",
    "        if method == \"downsample\":\n",
    "            # ð¹ Basic Downsampling: Keep `N` samples per overrepresented tissue\n",
    "            for tissue, indices in group_idx.items():\n",
    "                sampled_idx = resample(indices, n_samples=min(N, len(indices)), random_state=42, replace=False)\n",
    "                balanced_idx.extend(sampled_idx)\n",
    "\n",
    "        elif method == \"stratified\":\n",
    "            # ð¹ Stratified Downsampling: Keep `N` samples per overrepresented tissue while preserving variance\n",
    "            pca = PCA(n_components=n_components)\n",
    "            pca_fit = pca.fit_transform(data[\"X\"])\n",
    "\n",
    "            for tissue, indices in group_idx.items():\n",
    "                if len(indices) > N:\n",
    "                    # Rank by PCA variance and keep highest N\n",
    "                    tissue_variances = np.var(pca_fit[indices], axis=1)\n",
    "                    selected_idx = np.argsort(tissue_variances)[-N:]\n",
    "                    sampled_idx = np.array(indices)[selected_idx]\n",
    "                else:\n",
    "                    sampled_idx = indices  # Keep as is if already <= N\n",
    "                balanced_idx.extend(sampled_idx)\n",
    "\n",
    "        # Store downsampled indexes for this drug\n",
    "        group_idx_balanced[drug] = balanced_idx\n",
    "\n",
    "        # Apply downsampling to the dataset\n",
    "        data_dict[drug][\"Balanced_Metadata\"] = mdata.loc[balanced_idx].copy()\n",
    "        data_dict[drug][\"Balanced_X\"] = data[\"X\"].loc[balanced_idx].copy()\n",
    "        data_dict[drug][\"Balanced_Y\"] = data[\"Y\"].loc[balanced_idx].copy()\n",
    "        data_dict[drug][\"Balanced_OBS\"] = data[\"OBS\"].loc[balanced_idx].copy()\n",
    "\n",
    "        if apply_smote:\n",
    "            # ð¹ Apply SMOTE only to underrepresented tissues\n",
    "            smote = SMOTE(sampling_strategy=\"auto\", random_state=42)\n",
    "            try:\n",
    "                X_resampled, Y_resampled = smote.fit_resample(data_dict[drug][\"Balanced_X\"], data_dict[drug][\"Balanced_Y\"])\n",
    "                data_dict[drug][\"Balanced_X\"] = X_resampled\n",
    "                data_dict[drug][\"Balanced_Y\"] = Y_resampled\n",
    "            except ValueError:\n",
    "                warnings.warn(f\"SMOTE failed for {drug} due to insufficient samples. Skipping SMOTE.\")\n",
    "\n",
    "    return data_dict, group_idx_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eccccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "data = training_sets_dict[\"Gefitinib\"][\"Metadata\"]\n",
    "maj_class = data[data[\"Class\"] == \"over\"].index\n",
    "min_class = data[data[\"Class\"] == \"down\"].index\n",
    "min_data=data[data.index.isin(min_class)]\n",
    "maj_data=data[data.index.isin(maj_class)]\n",
    "MU_min=np.ceil(min_data[\"OncotreeSubtype\"].value_counts().mean()).astype(int)\n",
    "MU_maj=np.ceil(maj_data[\"OncotreeSubtype\"].value_counts().mean()).astype(int)\n",
    "N = MU_maj MU_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f469ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_idx_downsampled = {}\n",
    "group_idx = maj_data.groupby(\"OncotreeSubtype\").groups\n",
    "group_idx_downsampled = {gn: resample(group, replace=False, n_samples=N, random_state=42) for gn, group in group_idx.items()}\n",
    "group_idx_downsampled = [idx for sublist in group_idx_downsampled.values() for idx in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afc0062",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_training_dict = {}\n",
    "for drug, data in training_sets_dict.items():\n",
    "    LEFTOVER=data[\"OBS\"][data[\"OBS\"].index.isin(test[drug])]\n",
    "    SAMPLES=data[\"OBS\"][~data[\"OBS\"].index.isin(test[drug])]\n",
    "    GEX=data[\"X\"][~data[\"X\"].index.isin(test[drug])]\n",
    "    RESP=data[\"Y\"][~data[\"Y\"].index.isin(test[drug])]\n",
    "    META=data[\"Metadata\"][~data[\"Metadata\"].index.isin(test[drug])]\n",
    "    resampled_training_dict[drug] = {\"LEFT\":LEFTOVER, \"OBS\":SAMPLES, \"X\":GEX, \"Y\":RESP, \"Metadata\":META}  # i leftover gli injectiamo quando traininamo dentro l'inner loop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b85ff7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CategorizeOTS(resampled_training_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924d11a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "over_class = [\"Lung Adenocarcinoma\", \"Small Cell Lung Cancer\", \"Colon Adenocarcinoma\"]\n",
    "\n",
    "downsampled_data_dict = {}\n",
    "\n",
    "X_data = training_sets[\"AZD6738\"][\"X\"]\n",
    "Y_data = training_sets[\"AZD6738\"][\"Y\"]\n",
    "\n",
    "dat_ots = training_sets[\"AZD6738\"][\"Metadata\"][\"OncotreeSubtype\"]\n",
    "\n",
    "for t in over_class:\n",
    "    sub_ots = dat_ots[(dat_ots == t)]\n",
    "    K = int(np.ceil(len(sub_ots)/2))\n",
    "    random.seed(42)\n",
    "    idx_sampled = random.sample(list(sub_ots.index),K)\n",
    "    X_data = X_data[~X_data.index.isin(idx_sampled)]\n",
    "    Y_data = Y_data[~Y_data.index.isin(idx_sampled)]\n",
    "    #X = X_data.fillna(0).to_numpy()\n",
    "    #Y = Y_data.fillna(Y_data.mean()).to_numpy()\n",
    "    #X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n",
    "    \n",
    "    print(f\"X train:{X_train.shape}, \" f\"X_test:{X_test.shape}, \" f\"Y_train:{Y_train.shape}, \"f\"Y_test:{Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adef7d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "outer = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb843fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1590ac1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf5b0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Dictionary to store trained models & scores\n",
    "models = {}\n",
    "\n",
    "# Define number of folds for nested cross-validation\n",
    "n_outer_folds = 5  # Outer loop for model evaluation\n",
    "n_inner_folds = 5  # Inner loop for hyperparameter tuning\n",
    "\n",
    "# Outer cross-validation loop\n",
    "kf_outer = KFold(n_splits=n_outer_folds, shuffle=True, random_state=42)\n",
    "\n",
    "for dname, data in training_sets.items():\n",
    "    print(f\"Training LASSO model with Nested CV for {dname}...\")\n",
    "    for t in over_class:\n",
    "        sub_ots = dat_ots[(dat_ots == t)]\n",
    "        K = int(np.ceil(len(sub)/2))\n",
    "        random.seed(42)\n",
    "        idx_sampled = random.sample(list(sub_ots.index),K)\n",
    "        X_data = X_data[~X_data.index.isin(idx_sampled)]\n",
    "        Y_data = Y_data[~Y_data.index.isin(idx_sampled)]\n",
    "        X = X_data.fillna(0).to_numpy()\n",
    "        Y = Y_data.fillna(Y_data.mean()).to_numpy()\n",
    "\n",
    "        # Outer loop: Perform k-fold cross-validation\n",
    "        outer_r2_scores = []\n",
    "        outer_rmse_scores = []\n",
    "    \n",
    "        for train_idx, test_idx in kf_outer.split(X, Y): \n",
    "            # Split dataset into train-test sets\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "            # Normalize features using training data statistics\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)  # Use same scaling for test data\n",
    "\n",
    "            # Inner loop: Perform hyperparameter tuning within the training set\n",
    "            kf_inner = KFold(n_splits=n_inner_folds, shuffle=True, random_state=42)\n",
    "            lasso_cv = LassoCV(alphas=np.logspace(-3, 1, 50), cv=kf_inner, random_state=42, n_jobs=-1)\n",
    "            lasso_cv.fit(X_train_scaled, Y_train)\n",
    "            best_alpha = lasso_cv.alpha_\n",
    "\n",
    "        # Train final model with best alpha on the entire training set\n",
    "            final_model = LassoCV(alphas=[best_alpha], cv=5, random_state=42)\n",
    "            final_model.fit(X_train_scaled, Y_train)\n",
    "\n",
    "        # Predict on the test set\n",
    "        Y_pred = final_model.predict(X_test_scaled)\n",
    "\n",
    "        # Evaluate model\n",
    "        mse = mean_squared_error(Y_test, Y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "        # Store outer fold scores\n",
    "        outer_r2_scores.append(r2)\n",
    "        outer_rmse_scores.append(rmse)\n",
    "\n",
    "    # Store model, weights, and scaler parameters\n",
    "    models[dname] = {\n",
    "        \"coef\": final_model.coef_,\n",
    "        \"intercept\": final_model.intercept_,\n",
    "        \"scaler_mean\": scaler.mean_,\n",
    "        \"scaler_scale\": scaler.scale_,\n",
    "        \"best_alpha\": best_alpha,\n",
    "        \"outer_r2_scores\": outer_r2_scores,\n",
    "        \"mean_r2\": np.mean(outer_r2_scores),\n",
    "        \"std_r2\": np.std(outer_r2_scores),\n",
    "        \"outer_rmse_scores\": outer_rmse_scores,\n",
    "        \"mean_rmse\": np.mean(outer_rmse_scores),\n",
    "        \"std_rmse\": np.std(outer_rmse_scores),\n",
    "    }\n",
    "\n",
    "    # Save model\n",
    "    with open(f\"lasso_model_{dname}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(models[dname], f)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"{dname}: Mean RÂ² = {np.mean(outer_r2_scores):.4f} Â± {np.std(outer_r2_scores):.4f} | \"\n",
    "          f\"Mean RMSE = {np.mean(outer_rmse_scores):.4f} Â± {np.std(outer_rmse_scores):.4f} | Best Alpha: {best_alpha:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete for all drugs!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76779a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Dictionary to store trained models & scores\n",
    "models = {}\n",
    "\n",
    "# Define number of folds for cross-validation\n",
    "\n",
    "outer = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for dname, data in training_sets.items():\n",
    "    \n",
    "    print(f\"Training LASSO model with Nested CV for {dname}...\")\n",
    "    \n",
    "    X = data[\"X\"].fillna(0).to_numpy()\n",
    "    Y = data[\"Y\"].fillna(Y.mean()).to_numpy()\n",
    "    \n",
    "    # Split train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Inner loop for hyperparameter tuning (runs in parallel)\n",
    "    lasso_cv = LassoCV(alphas=np.logspace(-3, 1, 50), cv=5, random_state=42, n_jobs=-1)\n",
    "\n",
    "    # Perform nested cross-validation\n",
    "    r2_scores = cross_val_score(lasso_cv, X_scaled, Y, scoring='r2', cv=outer, n_jobs=-1)\n",
    "    rmse_scores = np.sqrt(-cross_val_score(lasso_cv, X_scaled, Y, scoring='neg_mean_squared_error', cv=outer, n_jobs=-1))\n",
    "\n",
    "    # Train final model on full dataset\n",
    "    lasso_cv.fit(X_scaled, Y)\n",
    "    best_alpha = lasso_cv.alpha_\n",
    "\n",
    "    # Store model, weights, and scaler parameters\n",
    "    models[dname] = {\n",
    "        \"coef\": lasso_cv.coef_,\n",
    "        \"intercept\": lasso_cv.intercept_,\n",
    "        \"scaler_mean\": scaler.mean_,\n",
    "        \"scaler_scale\": scaler.scale_,\n",
    "        \"best_alpha\": best_alpha,\n",
    "        \"cv_r2_scores\": r2_scores,\n",
    "        \"mean_r2\": np.mean(r2_scores),\n",
    "        \"std_r2\": np.std(r2_scores),\n",
    "        \"cv_rmse_scores\": rmse_scores,\n",
    "        \"mean_rmse\": np.mean(rmse_scores),\n",
    "        \"std_rmse\": np.std(rmse_scores),\n",
    "    }\n",
    "\n",
    "    # Save model\n",
    "    with open(f\"lasso_model_{dname}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(models[dname], f)\n",
    "\n",
    "    print(f\"{dname}: Mean RÂ² = {np.mean(r2_scores):.4f} Â± {np.std(r2_scores):.4f} | \"\n",
    "          f\"Mean RMSE = {np.mean(rmse_scores):.4f} Â± {np.std(rmse_scores):.4f} | Best Alpha: {best_alpha:.4f}\")\n",
    "\n",
    "print(\"\\nComplete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (scanpy 3rd)",
   "language": "python",
   "name": "scanpy_backup_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
